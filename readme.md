The traditional RL algorithms are evaluated on a 4x4 grid world environment as shown below. An agent is allowed to move to up,down,left, and right to an ajacient grid. Each action takes has a cost of 1. When the agent is at the boundary, for example at state 2, if it takes the action of going up, the action will have no effects but still has a cost of 1. 
![reinforcement_learning](assets/grid_world.jpg)

All state value and state action values are initialized to 0. After 500 iterations, states 1,4,5,8,12,13,14 have converged.
![reinforcement_learning](assets/progress_500.jpg)

After 1000 iterations, all state values converged.
![reinforcement_learning](assets/progress_1000.jpg)

After 2000 iterations, all state values and state action values have converged.
